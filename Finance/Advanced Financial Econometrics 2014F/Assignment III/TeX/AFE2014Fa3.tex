\documentclass{article}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{float}
  \usepackage{setspace}
  \usepackage{stata}
  \usepackage{natbib}
  \usepackage{indentfirst}
  \usepackage{listings}
  \usepackage{textcomp}
  \usepackage{color}
  \usepackage{fontenc}
  \usepackage{accsupp}% http://ctan.org/pkg/accsupp
  \usepackage[colorlinks, linkcolor=NavyBlue, anchorcolor=Maroon, citecolor=TealBlue]{hyperref}%For hyper-link reference
    \input{Stata.tex}% for stata language highlights
  \usepackage{mdframed}
\setlength{\parindent}{2em}

\topmargin=-1.2cm \oddsidemargin=0.1cm \evensidemargin=0.1cm
\textwidth=16 true cm \textheight=23 true cm

\font\euler=EUSM10 \font\eulers=EUSM7

\begin{document}

\title{Financial Econometrics\\Assignment $3^{\text{rd}}$}
\author{{\normalsize SHENG Hao, 1401211818, via \LaTeX}}
\date{\today}

\maketitle

\def \Pr{{\rm Pr}}
\baselineskip 0.6cm

 This assignment is for the graduate course Financial Econometrics at GSM(Guanghua School of Management, Peking Univ.). 


\section{Q1}
Given the AR equation, we have its characteristic equation:
\begin{equation}
  x^2-1.4x+0.55=0
\end{equation}
Thus $x \doteq 0.7\pm0.244949 i$.
Since $|x|<1$, this is a stationary process. And because it has two conjugate imaginary roots, it has a cycle $|\frac{2 \pi} {Arg(x)}| \doteq 18.6661$


\section{Q2}
The estimation model is:
\begin{equation}
  (1-\phi_1 L - \phi_4 L^4)y_t = \phi_0 + (1+\theta_1 L + \theta_3 L^3)\epsilon_t
\end{equation}
Take care of the estimating result we got, the reduced form of the above equation's estimation would be:
\begin{equation}
  y_t=0.00554+0.83872y_{t-1}+0.0256y_{t-4}+\epsilon_{t}-0.77072\epsilon_{t-1} +0.04392\epsilon_{t-3}
\end{equation}
\section{Q3}
We applied t-test(to OLS estimation) to see if this process has a unit root. With the information provided, we calculated the t-statistics of X and Y, for scenarios with or without 3-order lagged terms:
\begin{table}[!h]
\centering
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{tabular}{r|rr}

           &          X &          Y \\
\hline
\hline
Without lagged terms &      -3.17 &       -1.22 \\

With 3 order lagged terms &       -1.45 &      -1.625 \\
\hline
\end{tabular}  
\caption{t-statistics estimated}
\end{table}

Refer to the t-statistics distribution(for DOF=250), we can conclude:
\begin{itemize}
  \item X: With a significant level of 0.1, we cannot refuse zero hypothesis that there is a unit root for model with a 3-order lagged term. However, we can refuse the zero hypothesis for model without any lagged terms at a level of 0.01. That is, we can say X is stationary for model without lagged terms at a significant level 1\%.
  \item Y: With a significant level of 0.05, we cannot refuse zero hypothesis that there is a unit root for models either with a 3-order lagged term or without any lagged terms. That is, we cannot say Y is stationary for either model at a significant level 5\%.
\end{itemize}

\begin{table}[!h]
\centering
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{tabular}{rrrr}

Significant level  &          0.1 &          0.05  & 0.01\\
\hline
\hline
t-statistics &     -1.62 &        -1.95 &-2.58\\

\hline
\end{tabular}  
\caption{DF distribution, DOF=250}
\end{table}


\section{Q4}
\subsection{Data}
We start from the monthly chain relative ratio. After several steps of adjustments, we research the quarterly quarterly constant price CPI (Dec. 1999 = 100):
\begin{figure}[!h]
\centering
\includegraphics[width = 0.6\linewidth]{CPI.png}
\caption{Quarterly CPI (Dec. 1999 = 100)}
\end{figure}

\subsection{Unit-root test and Difference}
A direct D-F test reports:
\begin{table}[!ht]
\centering
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{tabular}{rrrrrrrr}

{\bf Type} & {\bf Lags} &  {\bf Rho} & {\bf Pr $<$ Rho} &  {\bf Tau} & {\bf Pr $<$ Tau} &    {\bf F} & {\bf Pr $>$ F} \\
\hline
\hline
{\bf Zero Mean} &    {\bf 0} &      0.252 &     0.7373 &       3.19 &     0.9995 &            &            \\

    {\bf } &    {\bf 1} &     0.2622 &     0.7396 &       3.25 &     0.9996 &            &            \\

    {\bf } &    {\bf 2} &     0.2568 &     0.7381 &       3.11 &     0.9993 &            &            \\

    {\bf } &    {\bf 3} &     0.2557 &     0.7377 &       2.73 &      0.998 &            &            \\

{\bf Single Mean} &    {\bf 0} &     0.7705 &     0.9819 &       0.82 &     0.9934 &       5.17 &     0.0419 \\

    {\bf } &    {\bf 1} &     0.5946 &     0.9772 &       0.67 &     0.9901 &       5.26 &     0.0395 \\

    {\bf } &    {\bf 2} &      0.664 &      0.979 &        0.8 &      0.993 &       4.88 &     0.0494 \\

    {\bf } &    {\bf 3} &     0.5869 &     0.9768 &       0.67 &     0.9901 &       3.71 &     0.1559 \\

{\bf Trend} &    {\bf 0} &   -11.7725 &     0.2779 &      -3.22 &     0.0934 &       6.63 &     0.0546 \\

    {\bf } &    {\bf 1} &   -10.9738 &     0.3242 &      -2.75 &     0.2234 &       4.68 &     0.2754 \\

    {\bf } &    {\bf 2} &   -12.0655 &     0.2596 &       -2.9 &     0.1722 &       5.34 &     0.1504 \\

    {\bf } &    {\bf 3} &   -15.6624 &     0.1154 &      -3.08 &     0.1246 &       5.67 &     0.0974 \\
\hline
\end{tabular}  
\caption{Augmented Dickey-Fuller Unit Root Tests for Original Data}
\end{table}
\newpage
Take care of Tau, we cannot refuse hypothesis that there is a unit root. We use the 1-order difference of the log term:

\begin{table}[!ht]
\centering
% Table generated by Excel2LaTeX from sheet 'DF2'
\begin{tabular}{rrrrrrrr}

{\bf Type} & {\bf Lags} &  {\bf Rho} & {\bf Pr $<$ Rho} &  {\bf Tau} & {\bf Pr $<$ Tau} &    {\bf F} & {\bf Pr $>$ F} \\
\hline
\hline
{\bf Zero Mean} &    {\bf 0} &    -39.473 &     $<$.0001 &      -5.88 &     $<$.0001 &            &            \\

    {\bf } &    {\bf 1} &   -33.2666 &     $<$.0001 &      -3.96 &     0.0002 &            &            \\

    {\bf } &    {\bf 2} &   -18.2928 &     0.0015 &      -2.61 &     0.0103 &            &            \\

    {\bf } &    {\bf 3} &    -6.1125 &     0.0823 &       -1.5 &     0.1229 &            &            \\

{\bf Single Mean} &    {\bf 0} &   -47.6372 &     0.0004 &      -7.16 &     0.0001 &      25.68 &      0.001 \\

    {\bf } &    {\bf 1} &   -58.8798 &     0.0003 &      -5.33 &     0.0001 &       14.2 &      0.001 \\

    {\bf } &    {\bf 2} &   -50.8383 &     0.0003 &      -3.88 &     0.0044 &       7.56 &      0.001 \\

    {\bf } &    {\bf 3} &   -19.2502 &     0.0068 &       -2.5 &     0.1229 &       3.15 &     0.2916 \\

{\bf Trend} &    {\bf 0} &   -48.8467 &     $<$.0001 &      -7.23 &     $<$.0001 &      26.14 &      0.001 \\

    {\bf } &    {\bf 1} &    -64.972 &     $<$.0001 &      -5.48 &     0.0003 &      15.01 &      0.001 \\

    {\bf } &    {\bf 2} &   -61.6708 &     $<$.0001 &      -4.01 &     0.0157 &       8.06 &      0.021 \\

    {\bf } &    {\bf 3} &   -22.1141 &     0.0207 &       -2.5 &     0.3254 &        3.2 &     0.5552 \\
\hline
\end{tabular}  
\caption{Augmented Dickey-Fuller Unit Root Tests for 1-order difference log term}
\end{table}
This table justify our processing of data.

\subsection{Parameters of ARMA}
From the following figures of ACF and PACF we can see: ACF and PACF get close to zero swiftly; there isn't a geometry attenuation in ACF; PACF is very close to zero after the first term.
\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{gplot1.png}
\caption{ACF}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{gplot2.png}
\caption{PACF}
\end{figure}

\newpage
\subsection{Specification and Results}
Due to the analysis, we will look into ARMA(1,1), ARMA(1,(1,4)), ARMA(2,1), ARMA(2,(1,4)). The result follows:
\begin{table}[!ht]
\centering
% Table generated by Excel2LaTeX from sheet 'result'
\begin{tabular}{ccccc}
\hline
\hline
           &        (1) &        (2) &        (3) &        (4) \\

           &    p=1 q=1 &  p=1 q=1,4 &    p=2 q=1 &  p=2 q=1,4 \\
\hline
        a0 & 4.62773*** & 4.63122*** & 4.64097*** & 4.64853*** \\

           &   (0.001314) &    (0.01213) &    (0.01393) &    (0.01345) \\

        a1 &       1*** &          1 &    1.15363 & 1.51351*** \\

           &   *0.002221) &     (0.0256) &    (1.38178) &    (0.43043) \\

        a2 &            &            &   -0.15363 &   -0.51351 \\

           &            &            &    (1.39137) &    (0.44167) \\

        β1 &   -0.18016 &   -0.16841 &    0.03335 &    0.29564 \\

           &    (0.15633) &    (0.15082) &    (1.39005) &    (0.43939) \\

        β4 &            & -0.28409** &            &   -0.23303 \\

           &            &    (0.15114) &            &    (0.16061) \\
\hline
       AIC &    269.468 &    262.289 &    263.917 &    262.82 \\

       SBC &    263.918 &    254.888 &    256.516 &    253.569 \\

         6 &       9.61 &       3.63 &       8.71 &      3.64 \\

        12 &      22.59 &      11.89 &      19.16 &      9.66 \\

        18 &      38.65 &      26.12 &      31.28 &      20.4 \\
\hline
\hline
\end{tabular}  
\caption{Result of ARMA}
\end{table}

Since the Granger-Newbold statistics is -0.4763, which is not significant at 5\% level. We'd like to report the result of ARMA(1,(1,4)).

 \newpage
 \begin{appendix}\label{sec:appendix}
 \section{Appendix}
 \subsection{Stata Do-file}
  \lstinputlisting[language=stata, style=stata-editor]{stata.txt}
 \newpage
 \subsection{SAS Code-file}
  \lstinputlisting[language=SAS, style = stata-editor]{SAS.txt}
% \lstinputlisting[language=stata, style=stata-editor]{AFE2014Fa2.txt}
% \nocite{*}
% \bibliography{Library}
% \bibliographystyle{jpe}
 \end{appendix}
\end{document}
